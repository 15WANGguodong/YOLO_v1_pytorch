{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Yolo_pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b1209ce6840045dd8b30ad354299c027": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c36ef40425a348c6a2bad9795d94cea9",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_fb1731c47bc249d6b64d12685f13a396",
              "IPY_MODEL_d3d8c8cc5b7144438a1bcad959d25c6f"
            ]
          }
        },
        "c36ef40425a348c6a2bad9795d94cea9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fb1731c47bc249d6b64d12685f13a396": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f2d29f253e1b4b4fa1407ec79fd4f9ef",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 553433881,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 553433881,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5ec4e44cdabc4eb8b5857c57aa4fcadd"
          }
        },
        "d3d8c8cc5b7144438a1bcad959d25c6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_6b0162ccd65d44bbab1c31f3cc72117b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 528M/528M [00:41&lt;00:00, 13.4MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_19250e998e58496bb5ceb9bfb5b4c710"
          }
        },
        "f2d29f253e1b4b4fa1407ec79fd4f9ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5ec4e44cdabc4eb8b5857c57aa4fcadd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6b0162ccd65d44bbab1c31f3cc72117b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "19250e998e58496bb5ceb9bfb5b4c710": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utQNYsfDRWtc",
        "colab_type": "text"
      },
      "source": [
        "#### Set up google colab and unzip train data¶\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oy_u91wAB9v5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set up google drive in google colab\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydNJzNLQCIvX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Unzip training data from drive\n",
        "\n",
        "!unzip -q 'drive/My Drive/VOCdevkit.zip'"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klzAYtrZkUIT",
        "colab_type": "text"
      },
      "source": [
        "####Import Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfaVZ852B04O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import math\n",
        "import xml.etree.ElementTree as ET\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from PIL import Image,ImageFilter, ImageEnhance\n",
        "import cv2\n",
        "import torch.optim as optim\n",
        "from torchvision.ops import nms\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import random"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PG6bQVilkZMA",
        "colab_type": "text"
      },
      "source": [
        "#### Define Model Hyper Parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "982FS4JECC1e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7c673560-8767-4857-8d13-a2538d84aa8e"
      },
      "source": [
        "#Classes to train on\n",
        "select_classes = {'person', 'bird', 'cat', 'cow', 'dog', 'horse', 'sheep', 'aeroplane', 'bicycle', 'boat', 'bus', 'car', 'motorbike', 'train', 'bottle', 'chair', 'diningtable', 'pottedplant', 'sofa', 'tvmonitor'}\n",
        "\n",
        "input_image_height = 448\n",
        "input_image_width = 448\n",
        "anchors_per_box = 2  # terminology anchors used here is incorrect - it should be boxes_per_grid instead\n",
        "\n",
        "grid_ht = int(input_image_height/7)\n",
        "grid_wt = int(input_image_width/7)\n",
        "\n",
        "\n",
        "prob_threshold = 0.1\n",
        "conf_threshold = 0.1\n",
        "nms_threshold = 0.5\n",
        "\n",
        "batch_num = 8\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdSp2W2Ekull",
        "colab_type": "text"
      },
      "source": [
        "####Visualisation Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QvaJHA2JktLo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Given input image, draw rectangles as specified by gt_box and pred_box and display\n",
        "def visualize_tensor(img, gt_box, pred_box, pred_labels = '', pred_score = ''):\n",
        "    plt.figure(figsize=(5,5))\n",
        "    transform_img = inv_normalize(img[0]).permute(1,2,0).to('cpu').numpy()\n",
        "    transform_img = transform_img.copy()\n",
        "    for box in gt_box:\n",
        "        x0, x1, y0, y1 = box\n",
        "        cv2.rectangle(transform_img, (int(x0),int(y0)), (int(x1),int(y1)), color=(0, 255, 255), thickness=2)\n",
        "    \n",
        "    if pred_labels == '':\n",
        "      for box in pred_box:\n",
        "          x0, x1, y0, y1 = box\n",
        "          cv2.rectangle(transform_img, (int(x0), int(y0)), (int(x1), int(y1)), color=(255, 0, 0), thickness=2)\n",
        "    else:\n",
        "      for idx,box in enumerate(pred_box):\n",
        "          x0, x1, y0, y1 = box\n",
        "          cv2.rectangle(transform_img, (int(x0), int(y0)), (int(x1), int(y1)), color=(255, 0, 0), thickness=2)\n",
        "          print( pred_score[idx].item)\n",
        "          cv2.putText(transform_img, pred_labels[idx], (x0, y0+15), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
        "          cv2.putText(transform_img, str(pred_score[idx].item())[:4], (x0, y0+40), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
        "\n",
        "    plt.imshow(transform_img)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFrlN0lZkg2Y",
        "colab_type": "text"
      },
      "source": [
        "####Handle Training Data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fzd24IErkk0L",
        "colab_type": "text"
      },
      "source": [
        "#####Get all classes and create label encoding\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZLArShrDOQR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "84827207-d4f9-40fd-e401-878f7ad57d0c"
      },
      "source": [
        "# Getting all the classes and creating label encoding\n",
        "\n",
        "all_labels = []\n",
        "max_gt_count = -1   # A single image in training set has max these many classes\n",
        "\n",
        "for out in sorted(os.listdir('VOCdevkit/VOC2007/Annotations/')):\n",
        "    tree = ET.parse('VOCdevkit/VOC2007/Annotations/' + out)\n",
        "    current_gt_count = 0\n",
        "    for obj in tree.findall('object'):\n",
        "        lab = (obj.find('name').text)\n",
        "        # all_labels.append(lab)\n",
        "        if (lab in (select_classes)):\n",
        "          all_labels.append(lab)\n",
        "          current_gt_count += 1\n",
        "    max_gt_count = max(current_gt_count, max_gt_count)\n",
        "\n",
        "     \n",
        "\n",
        "distict_labels = list(set(all_labels))\n",
        "distict_labels = sorted(distict_labels)\n",
        "\n",
        "lab_to_val = {j:i for i,j in enumerate(distict_labels)}\n",
        "val_to_lab = {i:j for i,j in enumerate(distict_labels)}\n",
        "\n",
        "num_classes = len(distict_labels)\n",
        "\n",
        "print(\"All Labels -- \",np.unique(all_labels, return_counts=True)[0])\n",
        "print(\"Label Counts -- \",np.unique(all_labels, return_counts=True)[1]) \n",
        "print(\"Num Classes -- \",num_classes)\n",
        "print(\"Max Detection in an image -- \",max_gt_count)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All Labels --  ['aeroplane' 'bicycle' 'bird' 'boat' 'bottle' 'bus' 'car' 'cat' 'chair'\n",
            " 'cow' 'diningtable' 'dog' 'horse' 'motorbike' 'person' 'pottedplant'\n",
            " 'sheep' 'sofa' 'train' 'tvmonitor']\n",
            "Label Counts --  [ 331  418  599  398  634  272 1644  389 1432  356  310  538  406  390\n",
            " 5447  625  353  425  328  367]\n",
            "Num Classes --  20\n",
            "Max Detection in an image --  42\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEysnPYTk-Jz",
        "colab_type": "text"
      },
      "source": [
        "#####Create lists with train and test images "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zcc24sG-khEY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file = open('drive/My Drive/val.txt', \"r\")\n",
        "valid_images = file.read().split('\\n')\n",
        "valid_images = valid_images[:-1]\n",
        "\n",
        "\n",
        "train_images = []\n",
        "for img in os.listdir('VOCdevkit/VOC2007/JPEGImages/'):\n",
        "  if img[:-4] in valid_images:\n",
        "    continue\n",
        "  train_images.append(img[:-4])\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGNTSAAZlInR",
        "colab_type": "text"
      },
      "source": [
        "##### Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSraWTqIbXpB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Random blur on training image\n",
        "def random_blur(img):\n",
        "  if random.random() < 0.5:\n",
        "    return img\n",
        "  \n",
        "  rad = random.choice([1,2])\n",
        "  img = img.filter(ImageFilter.BoxBlur(radius=rad))\n",
        "  return img\n",
        "\n",
        "\n",
        "# Random brightness, contrast, satutration and hue\n",
        "def random_color(img):\n",
        "  if random.random() < 0.1:\n",
        "    return img\n",
        "\n",
        "  img = transforms.ColorJitter(brightness=(0.5,2.0), contrast=(0.5,2.0), saturation=(0.5,2.0), hue=(-0.25,0.25))(img)\n",
        "  return img\n",
        "\n",
        "# Random horizontal flip\n",
        "def random_flip(img, gt_box):\n",
        "  if random.random() < 0.5:\n",
        "    return img,gt_box\n",
        "\n",
        "  img = transforms.RandomHorizontalFlip(p=1)(img)\n",
        "  temp = (gt_box[:,1]).copy()\n",
        "  gt_box[:,1] = img.size[0] - gt_box[:,0] #x1\n",
        "  gt_box[:,0] = img.size[0] - temp #x2\n",
        "\n",
        "  return img, gt_box\n",
        "\n",
        "\n",
        "# Random crop on image\n",
        "def random_crop(img, gt_box, labels):\n",
        "  if random.random() < 0.5:\n",
        "    return img,gt_box,labels\n",
        "  width, height = img.size\n",
        "  select_w = random.uniform(0.6*width, width)\n",
        "  select_h = random.uniform(0.6*height, height)\n",
        "\n",
        "  start_x = random.uniform(0,width - select_w)\n",
        "  start_y = random.uniform(0,height - select_h)\n",
        "\n",
        "  left = start_x\n",
        "  upper = start_y\n",
        "  right = start_x + select_w\n",
        "  bottom = start_y + select_h\n",
        "\n",
        "  gt_box_copy = gt_box.copy()\n",
        "\n",
        "  gt_box_copy[gt_box_copy[:,0] < left, 0] = left\n",
        "  gt_box_copy[gt_box_copy[:,1] > right, 1] = right\n",
        "  gt_box_copy[gt_box_copy[:,2] < upper, 2] = upper\n",
        "  gt_box_copy[gt_box_copy[:,3] > bottom, 3] = bottom\n",
        "\n",
        "  final_gt_box = []\n",
        "  final_labels = []\n",
        "\n",
        "  for i in range((gt_box_copy.shape[0])):\n",
        "    if (((gt_box_copy[i,1] - gt_box_copy[i,0])/(gt_box[i,1]-gt_box[i,0])) < 0.5):\n",
        "      continue\n",
        "    if (((gt_box_copy[i,3] - gt_box_copy[i,2])/(gt_box[i,3] - gt_box[i,2])) < 0.5):\n",
        "      continue\n",
        "    final_gt_box.append(gt_box_copy[i])\n",
        "    final_labels.append(labels[i])\n",
        "\n",
        "  if len(final_gt_box) == 0:\n",
        "    return img,gt_box,labels\n",
        "\n",
        "  final_gt_box = np.array(final_gt_box)\n",
        "  final_gt_box[:,0] = final_gt_box[:,0] - left\n",
        "  final_gt_box[:,1] = final_gt_box[:,1] - left\n",
        "  final_gt_box[:,2] = final_gt_box[:,2] - upper\n",
        "  final_gt_box[:,3] = final_gt_box[:,3] - upper\n",
        "\n",
        "  return img.crop((left, upper, right, bottom)), final_gt_box, final_labels"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_2PxpB9lOXs",
        "colab_type": "text"
      },
      "source": [
        "##### Creating pytorch dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBxupx6PCPtO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class pascal_voc_data(Dataset):\n",
        "    def __init__(self, img_dir,desc_dir, type_list,  isTrain=True,transform = None):\n",
        "        super().__init__()\n",
        "        self.img_dir = img_dir\n",
        "        self.desc_dir = desc_dir\n",
        "        self.type_list = type_list\n",
        "        self.isTrain = isTrain\n",
        "        self.transform = transform\n",
        "\n",
        "\n",
        "        self.img_names = []\n",
        "        self.img_descs = []\n",
        "        for img in sorted(os.listdir(img_dir)):\n",
        "          if img[:-4] in self.type_list:\n",
        "            self.img_names.append(img)\n",
        "        \n",
        "        for desc in sorted(os.listdir(desc_dir)):\n",
        "          if desc[:-4] in  self.type_list:\n",
        "            self.img_descs.append(desc)\n",
        "       \n",
        "        self.img_names = [os.path.join(img_dir, img_name) for img_name in self.img_names]\n",
        "        self.img_descs = [os.path.join(desc_dir, img_desc) for img_desc in self.img_descs]\n",
        "                \n",
        "        \n",
        "        self.loc_gts = []\n",
        "        self.loc_labels = []\n",
        "        self.final_img_names = []\n",
        "        for img_idx,img_desc in enumerate(self.img_descs):\n",
        "            tree = ET.parse(img_desc)\n",
        "            gt = []\n",
        "            loc_lab = []\n",
        "            for obj in tree.findall('object'):\n",
        "              if ((obj.find('name').text) not in (select_classes)):\n",
        "                continue\n",
        "\n",
        "              lab = lab_to_val[(obj.find('name').text)]\n",
        "              \n",
        "              loc1 = int(obj.find('bndbox').find('xmin').text)\n",
        "              loc2 = int(obj.find('bndbox').find('xmax').text)\n",
        "              loc3 = int(obj.find('bndbox').find('ymin').text)\n",
        "              loc4 = int(obj.find('bndbox').find('ymax').text)\n",
        "\n",
        "              # if ht or width is less than 10, ignore the gt box\n",
        "              if ((loc2 - loc1) < 10 ) or ((loc4 - loc3) < 10):\n",
        "                continue\n",
        "\n",
        "              gt.append([int(loc1),int(loc2),int(loc3),int(loc4)])\n",
        "              loc_lab.append(lab)\n",
        "            if (len(gt) == 0):\n",
        "              continue\n",
        "            self.loc_gts.append(gt)\n",
        "            self.loc_labels.append(loc_lab)\n",
        "            self.final_img_names.append(self.img_names[img_idx])\n",
        "\n",
        "        self.img_names = self.final_img_names\n",
        "             \n",
        "    def __len__(self):\n",
        "        return len(self.img_names)\n",
        "    \n",
        "    def __getitem__(self,idx):\n",
        "        img_name = self.img_names[idx]\n",
        "        img = Image.open(img_name)\n",
        "        arr_loc_gts = np.array(self.loc_gts[idx])\n",
        "        label = self.loc_labels[idx]\n",
        "\n",
        "        if self.isTrain:\n",
        "          img = random_blur(img)\n",
        "          img = random_color(img)\n",
        "          img,arr_loc_gts = random_flip(img,arr_loc_gts)\n",
        "          img,arr_loc_gts,label = random_crop(img,arr_loc_gts,label)\n",
        "\n",
        "        img_h_pre = img.size[1]\n",
        "        img_w_pre = img.size[0]\n",
        "\n",
        "        \n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "            \n",
        "        img_h_post = img.shape[1]\n",
        "        img_w_post = img.shape[2]\n",
        "        \n",
        "        height_ratio = img_h_post/img_h_pre\n",
        "        width_ratio = img_w_post/img_w_pre\n",
        "      \n",
        "        \n",
        "        \n",
        "        arr_loc_gts[:,0] = arr_loc_gts[:,0]*width_ratio\n",
        "        arr_loc_gts[:,1] = arr_loc_gts[:,1]*width_ratio\n",
        "        arr_loc_gts[:,2] = arr_loc_gts[:,2]*height_ratio\n",
        "        arr_loc_gts[:,3] = arr_loc_gts[:,3]*height_ratio\n",
        "                        \n",
        "        gts = (arr_loc_gts).tolist()\n",
        "        \n",
        "\n",
        "        num_gt = len(gts)\n",
        "        num_label = len(label)\n",
        "\n",
        "        if (num_gt != num_label):\n",
        "            raise Exception(\"invalid data - num_gt and num_labels do not match\")\n",
        "\n",
        "        fin_gt = torch.ones((max_gt_count,4))*(-1)\n",
        "        fin_lab = torch.ones((max_gt_count))*(-1)\n",
        "        count_gt = num_gt\n",
        "\n",
        "        gts = torch.FloatTensor(gts)\n",
        "        label = torch.FloatTensor(label)\n",
        "        fin_gt[:count_gt] = gts\n",
        "        fin_lab[:count_gt] = label\n",
        "        \n",
        "        return img, fin_gt,fin_lab.int(), count_gt\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gWGYktJlTun",
        "colab_type": "text"
      },
      "source": [
        "##### Define Transforms on train images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWqN4NLoDKD_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "While using pretrained models - \n",
        "Pytorch torchvision documentation - https://pytorch.org/docs/master/torchvision/models.html\n",
        "The images have to be loaded in to a range of [0, 1] and then \n",
        "normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]\n",
        "'''\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.Resize((input_image_height,input_image_width)),\n",
        "     transforms.ToTensor(),\n",
        "     transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
        "\n",
        "\n",
        "inv_normalize = transforms.Normalize(\n",
        "   mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],\n",
        "   std=[1/0.229, 1/0.224, 1/0.225]\n",
        ")"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsmJREtdlY9S",
        "colab_type": "text"
      },
      "source": [
        "##### Load dataset - train and valid"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhP-WqAf9MPr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "train_dataset = pascal_voc_data('VOCdevkit/VOC2007/JPEGImages/', 'VOCdevkit/VOC2007/Annotations/',train_images ,True,transform)\n",
        "valid_dataset = pascal_voc_data('VOCdevkit/VOC2007/JPEGImages/', 'VOCdevkit/VOC2007/Annotations/',valid_images ,False,transform)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sH-EpZ3uBGFN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c354cd6a-abc1-407e-ebc5-387b5d963c0c"
      },
      "source": [
        "print(len(train_dataset), len(valid_dataset))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4683 328\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lhs5uzjJlkMW",
        "colab_type": "text"
      },
      "source": [
        "#### YOLO Helper Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXq1eOuzloek",
        "colab_type": "text"
      },
      "source": [
        "##### In a 7 * 7 grid, check which grid the center of object lies in\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wK-IGLSsJziC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Input is single box in format (x1,x2,y1,y2)\n",
        "# returns grid where the center lies for this box\n",
        "# Verified Correctness\n",
        "def get_grid_for_gt_box(gt_box):\n",
        "    gt_box_tensor = torch.tensor(gt_box)\n",
        "    gt_ctr_x = (gt_box_tensor[1] + gt_box_tensor[0])//2\n",
        "    gt_ctr_y = (gt_box_tensor[2] + gt_box_tensor[3])//2\n",
        "\n",
        "    gt_grid_x = int(gt_ctr_x//grid_wt)\n",
        "    gt_grid_y = int(gt_ctr_y//grid_ht)\n",
        "    \n",
        "    return (gt_grid_x, gt_grid_y)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6S_Tbwulxz9",
        "colab_type": "text"
      },
      "source": [
        "##### Conversion between ground truth and prediction format\n",
        "\n",
        "\n",
        "\n",
        "*   Ground Truth - x1,x2,y1,y2\n",
        "*   Predicted - x,y,w,h (normalised wrt grid)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-xk3XB_0YqQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#grid_x and grid_y are start co ordinates for the grid\n",
        "# gt_box is the gt in format - x1,x2,y1,y2\n",
        "# converts gt_box in format - x,y,w,h - normalized\n",
        "\n",
        "def convert_gt_box_to_pred_format(gt_box, grid_x, grid_y):\n",
        "    out = torch.zeros(4)\n",
        "\n",
        "    gt_wt = gt_box[1] - gt_box[0]\n",
        "    gt_ht = gt_box[3] - gt_box[2]\n",
        "\n",
        "    out[0] = (gt_box[0] + (gt_wt/2) - grid_x)/(grid_wt*1.0) #x\n",
        "    out[1] = (gt_box[2] + (gt_ht/2) - grid_y)/(grid_ht*1.0) #y\n",
        "    out[2] = (gt_wt)/(input_image_width*1.0)  #w\n",
        "    out[3] = (gt_ht)/(input_image_height*1.0) #h\n",
        "    \n",
        "    return out\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#grid_x_start and grid_y_start are start co ordinates for the grid\n",
        "# pred_vector is vector corresponding to one of the box in grid - [x1,y1,w1,h1] - normalized\n",
        "# return output in format - x1,x2,y1,y2\n",
        "\n",
        "def convert_pred_vector_to_gt_format(pred_vector, grid_x_start, grid_y_start):\n",
        "\n",
        "    grid_box = torch.zeros(4)\n",
        "    out = torch.zeros(4)\n",
        "\n",
        "    \n",
        "    grid_box[0] = grid_x_start + grid_wt* pred_vector[0] #x\n",
        "    grid_box[1] = grid_y_start + grid_ht* pred_vector[1] #y\n",
        "    grid_box[2] = pred_vector[2]*input_image_width #w\n",
        "    grid_box[3] = pred_vector[3]*input_image_height #h\n",
        "\n",
        "    out[0] = grid_box[0] - (grid_box[2]/2)\n",
        "    out[1] = grid_box[0] + (grid_box[2]/2)\n",
        "    out[2] = grid_box[1] - (grid_box[3]/2)\n",
        "    out[3] = grid_box[1] + (grid_box[3]/2)\n",
        "\n",
        "    return out.int()\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0toWPptmbmH",
        "colab_type": "text"
      },
      "source": [
        "##### Given predicted Tensor as input, returns pred objects, scores and bounding box "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJYI36OuoZ6V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# input is prediction for an image (7*7*18)\n",
        "\n",
        "def decode_pred_data(preds):\n",
        "\n",
        "    #Get all the predicted locations and corresponding confidence -  98\n",
        "    pred_loc = preds[:,:,:10].reshape(-1,5)[:,:4]  \n",
        "    pred_conf = preds[:,:,:10].reshape(-1,5)[:,4]\n",
        "\n",
        "    #Corresponding to each grid, get the predicted object class score -  p(class/objec)\n",
        "    pred_class_score, pred_class = torch.max(preds[:,:,-1*num_classes:].reshape(-1,num_classes), dim=1)\n",
        "    pred_class_score = (((torch.stack((pred_class_score,pred_class_score))).T.reshape(-1,1))).T[0]     # Same score for both the boxes\n",
        "    pred_class = (((torch.stack((pred_class,pred_class))).T.reshape(-1,1))).T[0]     # Same class for both the boxes\n",
        "\n",
        "\n",
        "    \n",
        "    # P(class) = confidence * p(class/objec)\n",
        "    pred_prob = pred_conf*pred_class_score\n",
        "\n",
        "\n",
        "    # Only consider boxes with condition on prob_threshold and conf_threshold\n",
        "    keep_idx = []\n",
        "    keep_idx = (pred_prob > prob_threshold) & (pred_conf > conf_threshold)\n",
        "    pred_loc = pred_loc[keep_idx]\n",
        "    pred_conf = pred_conf[keep_idx]\n",
        "    pred_class = pred_class[keep_idx]\n",
        "    pred_class_score = pred_class_score[keep_idx]\n",
        "    pred_prob = pred_prob[keep_idx]\n",
        "\n",
        "    #start grid for the pred\n",
        "    pred_conf_grid = [((i//14),((i%14)//2)) for i in range(len(keep_idx)) if keep_idx[i]==True]\n",
        "\n",
        "\n",
        "    # convert to x1,x2,y1,y2 format\n",
        "    converted_preds = torch.zeros((len(pred_loc),4)) \n",
        "    for idx in range(len(pred_loc)):\n",
        "        converted_preds[idx] = convert_pred_vector_to_gt_format(pred_loc[idx], pred_conf_grid[idx][0]*grid_wt, pred_conf_grid[idx][1]*grid_ht)\n",
        "    converted_preds[converted_preds[:,0] < 0,0] = 0\n",
        "    converted_preds[converted_preds[:,1] > input_image_width,1] = input_image_width\n",
        "    converted_preds[converted_preds[:,2] < 0,2] = 0\n",
        "    converted_preds[converted_preds[:,3] > input_image_height,3] = input_image_height\n",
        "\n",
        "    fin_class = [val_to_lab[i.item()] for i in pred_class]\n",
        "    fin_score = pred_class_score\n",
        "    fin_conf = pred_conf\n",
        "    fin_prob = pred_prob\n",
        "\n",
        "    fin_class = np.array(fin_class)\n",
        "    fin_score = (fin_score.detach().to('cpu'))\n",
        "    fin_conf = (fin_conf.detach().to('cpu'))\n",
        "    fin_prob = (fin_prob.detach().to('cpu'))\n",
        "\n",
        "    final_conv_preds = []\n",
        "    final_conv_score = []\n",
        "    final_conv_class = []\n",
        "    \n",
        "    for cls in list(set(fin_class)):\n",
        "        keep_anchors = []\n",
        "        cur_class = fin_class[fin_class == cls]\n",
        "        cur_score = fin_score[fin_class == cls]\n",
        "        cur_conf = fin_conf[fin_class == cls]\n",
        "        cur_prob = fin_prob[fin_class == cls]\n",
        "        cur_preds = converted_preds[fin_class == cls]\n",
        "\n",
        "        sorted_class_scores = torch.argsort(cur_conf, descending=True) # Sorting wrt confidence\n",
        "        \n",
        "\n",
        "        while len(sorted_class_scores) > 1:\n",
        "            current = sorted_class_scores[0]\n",
        "            keep_anchors.append(current)\n",
        "            iou_matrix = get_iou_matrix(cur_preds[sorted_class_scores[1:]],cur_preds[current].reshape(1,-1,4)[0])\n",
        "            sorted_class_scores = sorted_class_scores[np.where(iou_matrix < nms_threshold)[0] + 1]\n",
        "        \n",
        "        if (len(sorted_class_scores) == 1):\n",
        "            keep_anchors.append(sorted_class_scores[0])\n",
        "\n",
        "        for k in keep_anchors:\n",
        "            final_conv_preds.append(cur_preds[k])\n",
        "            final_conv_score.append(cur_score[k])\n",
        "            final_conv_class.append(cur_class[k])\n",
        "\n",
        "    return final_conv_preds, final_conv_score, final_conv_class\n",
        "\n",
        "\n",
        "    "
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_j4hH54mmmXy",
        "colab_type": "text"
      },
      "source": [
        "##### Get IOU between 2 set of bounding boxes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWntXSX4zHRk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get Intersection over Union between all the boxes in pred_boxes and gt_boxes\n",
        "# Taken from faster_rcnn repo - https://github.com/pranayKD/faster_rcnn_colab_pytorch/blob/master/Faster_RCNN.ipynb\n",
        "\n",
        "def get_iou_matrix(pred_boxes, gt_boxes):\n",
        "\n",
        "    iou_matrix = torch.zeros((len(pred_boxes), len(gt_boxes)))\n",
        "    for idx,box in enumerate(gt_boxes):\n",
        "        if isinstance(box,torch.Tensor):\n",
        "          gt = torch.cat([box]*len(pred_boxes)).view(1,-1,4)[0]\n",
        "        else:\n",
        "          gt = torch.FloatTensor([box]*len(pred_boxes))\n",
        "        max_x = torch.max(gt[:,0],pred_boxes[:,0])\n",
        "        min_x = torch.min(gt[:,1],pred_boxes[:,1])\n",
        "        max_y = torch.max(gt[:,2],pred_boxes[:,2])\n",
        "        min_y = torch.min(gt[:,3],pred_boxes[:,3])\n",
        "                \n",
        "        invalid_roi_idx = (min_x < max_x) | (min_y < max_y)\n",
        "        roi_area = (min_x - max_x)*(min_y - max_y)\n",
        "        roi_area[invalid_roi_idx] = 0\n",
        "        \n",
        "        total_area = (gt[:,1] - gt[:,0])*(gt[:,3] - gt[:,2]) + \\\n",
        "                    (pred_boxes[:,1] - pred_boxes[:,0])*(pred_boxes[:,3]-pred_boxes[:,2]) - \\\n",
        "                     roi_area\n",
        "                    \n",
        "        iou = roi_area/(total_area + 1e-6)\n",
        "        \n",
        "        iou_matrix[:,idx] = iou\n",
        "    return iou_matrix"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJq5JBLHmrbQ",
        "colab_type": "text"
      },
      "source": [
        "##### For given predictions, returns the mask where object is present. Also, returns the ground truth tensor which is used in calculating loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rTlu0S_Ic6r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# takes input predicted matrix (7*7*18) , gt_locs and gt_labels\n",
        "# returns target matrix\n",
        "# returns mask for grids where object is present\n",
        "# for mask cannot simply take target matrix non zero condition, because there can be a 0 even at object loc\n",
        "\n",
        "def convert_format_pred_targets(preds, gts, labels):\n",
        "\n",
        "    final_targets = torch.zeros_like(preds)\n",
        "    obj_mask = torch.zeros_like(preds)\n",
        "\n",
        "    for gt_box_idx in range(len(gts)):\n",
        "        gt_box = gts[gt_box_idx]    \n",
        "        label = labels[gt_box_idx]\n",
        "\n",
        "        gt_grid_x, gt_grid_y = get_grid_for_gt_box(gt_box)  # Get grid location of the gt_box\n",
        "        start_x = grid_wt*gt_grid_x     # Get starting x co-ordinate of the box\n",
        "        start_y = grid_ht*gt_grid_y     # Get starting y co-ordinate of the box\n",
        "\n",
        "        pred_loc = preds[gt_grid_x,gt_grid_y][:5*anchors_per_box].contiguous().view(-1,5)[:,:4]     # Corresponding to the grid - get preds\n",
        "\n",
        "        temp_pred = torch.zeros((anchors_per_box,4))\n",
        "        for i in range(anchors_per_box):\n",
        "            temp_pred[i] = convert_pred_vector_to_gt_format(pred_loc[i], start_x, start_y)\n",
        "\n",
        "        temp_pred[temp_pred[:,0] < 0,0] = 0\n",
        "        temp_pred[temp_pred[:,2] < 0,2] = 0\n",
        "        temp_pred[temp_pred[:,1] > input_image_width,1] = input_image_width\n",
        "        temp_pred[temp_pred[:,3] > input_image_height,3] = input_image_height\n",
        "\n",
        "\n",
        "        iou = get_iou_matrix(temp_pred.to(device), [gt_box.to(device)]).to(device)   # get iou between preds and gt_box\n",
        "        iou_val = iou.max()\n",
        "        iou_idx = iou.argmax()\n",
        "\n",
        "        # visualize_tensor(a,[gt_box],temp_pred)\n",
        "\n",
        "        obj_mask[gt_grid_x,gt_grid_y,(5*iou_idx):(5*iou_idx+5)] = 1.0     # only consider box with max iou\n",
        "        obj_mask[gt_grid_x,gt_grid_y,(-1*num_classes):] = 1.0             # Consider classes for this box\n",
        "\n",
        "        final_targets[gt_grid_x,gt_grid_y,(5*iou_idx):(5*iou_idx+4)] = convert_gt_box_to_pred_format(gt_box, start_x, start_y)    # target locs for box is gt\n",
        "        final_targets[gt_grid_x,gt_grid_y,(5*iou_idx+4):(5*iou_idx+5)] = (iou_val)      # target conf for box is max iou value\n",
        "        final_targets[gt_grid_x,gt_grid_y,(anchors_per_box*5+label)] = 1.0  # assign target label value\n",
        "\n",
        "\n",
        "    return final_targets.to(device), obj_mask.to(device)\n",
        "\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXDAMeNS1anA",
        "colab_type": "text"
      },
      "source": [
        "#### Loss Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mh9isxM7m-1v",
        "colab_type": "text"
      },
      "source": [
        "##### Get loss for a single image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTmJm4eklaKG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_loss_one_image(preds, targets,obj_mask):\n",
        "\n",
        "\n",
        "    # Loss 1 - loss for predicted classes (if obj is present in grid)\n",
        "\n",
        "    obj_class_preds = (preds*obj_mask)[:,:,(-1*num_classes):]\n",
        "    obj_class_targets = (targets*obj_mask)[:,:,(-1*num_classes):]\n",
        "\n",
        "    loss1 = ((obj_class_preds - obj_class_targets)*(obj_class_preds - obj_class_targets)).sum()\n",
        "\n",
        "\n",
        "    ### TODO - for locs consider square root in w and h\n",
        "    # loss 2 - loss for box locations  (if obj is present in grid)\n",
        "    obj_locs_preds = (preds*obj_mask)[:,:,:(anchors_per_box*5)].contiguous().view(-1,5)[:,:4]\n",
        "    obj_locs_targets = (targets*obj_mask)[:,:,:(anchors_per_box*5)].contiguous().view(-1,5)[:,:4]\n",
        "\n",
        "    loss2 = ((obj_locs_preds - obj_locs_targets)*(obj_locs_preds - obj_locs_targets)).sum()\n",
        "\n",
        "\n",
        "    # loss 3 - confidence for box locations (if obj is present in grid)\n",
        "    obj_conf_preds = (preds*obj_mask)[:,:,:(anchors_per_box*5)].contiguous().view(-1,5)[:,4]\n",
        "    obj_conf_targets = (targets*obj_mask)[:,:,:(anchors_per_box*5)].contiguous().view(-1,5)[:,4]\n",
        "\n",
        "    loss3 = ((obj_conf_preds - obj_conf_targets)*(obj_conf_preds - obj_conf_targets)).sum()\n",
        "\n",
        "\n",
        "    # TODO - check this loss , even if there is a gt obj for grid do we need to consider loss for box with less IOU ? \n",
        "    # loss 3 - confidence for backgrounds (if obj is not present)\n",
        "\n",
        "    no_obj_mask = (1-obj_mask)\n",
        "    no_obj_conf_preds = (preds*no_obj_mask)[:,:,:(anchors_per_box*5)].contiguous().view(-1,5)[:,4]\n",
        "    no_obj_conf_targets = (targets*no_obj_mask)[:,:,:(anchors_per_box*5)].contiguous().view(-1,5)[:,4]\n",
        "   \n",
        "    loss4 = ((no_obj_conf_preds - no_obj_conf_targets)*(no_obj_conf_preds - no_obj_conf_targets)).sum()\n",
        "\n",
        "    \n",
        "    loss = loss1 + 5*loss2 + loss3 + 0.5*loss4\n",
        "\n",
        "    return loss\n",
        "    \n",
        "\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFTZec1enB4y",
        "colab_type": "text"
      },
      "source": [
        "##### Get loss for a batch of images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vI7jhNTgUcf-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_loss_batch(batch_preds, batch_gt_boxes_inp, batch_labels_inp, batch_count):\n",
        "\n",
        "\n",
        "    batch_size = batch_preds.shape[0]\n",
        "    loss = 0\n",
        "\n",
        "    for img in range(batch_size):\n",
        "        pred = batch_preds[img].clone().detach()\n",
        "        batch_gt_boxes = batch_gt_boxes_inp[img][:batch_count[img]]\n",
        "        batch_labels = batch_labels_inp[img][:batch_count[img]]\n",
        "        targets,obj_mask = convert_format_pred_targets(pred, batch_gt_boxes, batch_labels)\n",
        "\n",
        "        loss = loss + get_loss_one_image(batch_preds[img], targets,obj_mask)\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n",
        "\n",
        "    \n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v82qYq5qnFOX",
        "colab_type": "text"
      },
      "source": [
        "#### Define YOLO Model Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kR-pAv3nK2V",
        "colab_type": "text"
      },
      "source": [
        "##### Model base - vgg16 pretrained"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6TX_g4LDyof",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def base_model_vgg16(num_freeze_top): \n",
        "    vgg16 = models.vgg16(pretrained=True)\n",
        "    vgg_feature_extracter  = vgg16.features[:-1]\n",
        "    \n",
        "    # Freeze learning of top few conv layers\n",
        "    for layer in vgg_feature_extracter[:num_freeze_top]:\n",
        "        for param in layer.parameters():\n",
        "            param.requires_grad = False\n",
        "    \n",
        "    return vgg_feature_extracter.to(device)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ae1MfqHInOgy",
        "colab_type": "text"
      },
      "source": [
        "##### Add a few convolution layers, fully connected layer and reshape the preds "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBGFiFBhFUm6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class YOLONetwork(nn.Module):\n",
        "    def __init__(self, extractor):\n",
        "        super().__init__()\n",
        "        self.extractor = extractor\n",
        "        self.conv1 = nn.Conv2d(512, 1024,3,1,1)\n",
        "        self.pool1 = nn.MaxPool2d(2,2)\n",
        "        self.conv2 = nn.Conv2d(1024, 1024,3,1,1)\n",
        "        self.pool2 = nn.MaxPool2d(2,2)\n",
        "        self.lin1 = nn.Flatten()\n",
        "        self.drop1 = nn.Dropout(p=0.5)\n",
        "        self.lin2 = nn.Linear(7*7*1024, 7*7*(num_classes + anchors_per_box*5))\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "        out = self.extractor(x)\n",
        "        out = self.pool1(F.relu(self.conv1(out)))\n",
        "        out = self.pool2(F.relu(self.conv2(out)))\n",
        "        out = self.drop1(F.relu(self.lin1(out)))\n",
        "        out = torch.sigmoid(self.lin2(out))\n",
        "\n",
        "        num = out.shape[0]\n",
        "        return out.contiguous().view(num,7,7,-1)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T60Ym_OUBOJP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0ec88aad-44f3-4bbb-e02d-e06eff4c2fab"
      },
      "source": [
        "print('Training Data - %d & Valid Data %d '%(len(train_dataset), len(valid_dataset)))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Data - 4683 & Valid Data 328 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cN9RoXbNng7t",
        "colab_type": "text"
      },
      "source": [
        "#### Training Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmkOfl83nmHl",
        "colab_type": "text"
      },
      "source": [
        "##### Load a model from checkpoint or initiate a new model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xacSvQW5Et1K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "b1209ce6840045dd8b30ad354299c027",
            "c36ef40425a348c6a2bad9795d94cea9",
            "fb1731c47bc249d6b64d12685f13a396",
            "d3d8c8cc5b7144438a1bcad959d25c6f",
            "f2d29f253e1b4b4fa1407ec79fd4f9ef",
            "5ec4e44cdabc4eb8b5857c57aa4fcadd",
            "6b0162ccd65d44bbab1c31f3cc72117b",
            "19250e998e58496bb5ceb9bfb5b4c710"
          ]
        },
        "outputId": "ba726f28-6860-44ea-9acc-8c2f63325d1f"
      },
      "source": [
        "opt = 'Adam'\n",
        "load_model = ''\n",
        "\n",
        "extractor = base_model_vgg16(10)\n",
        "net = YOLONetwork(extractor).to(device)\n",
        "loss_hist = []\n",
        "valid_hist = []\n",
        "best_valid_loss = 100000\n",
        "if opt =='SGD':\n",
        "    optimizer = optim.SGD(net.parameters(), lr=0.001)\n",
        "if opt == 'Adam':\n",
        "    optimizer = optim.Adam(net.parameters(), lr=0.00001)\n",
        "\n",
        "epoch_start = 0\n",
        "\n",
        "if load_model != '':\n",
        "    print('loading model ... ')\n",
        "    checkpoint = torch.load(load_model, map_location=device)\n",
        "    net.load_state_dict(checkpoint['net_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    loss_hist = checkpoint['loss_hist']\n",
        "    valid_hist = checkpoint['valid_hist']\n",
        "    best_valid_loss = checkpoint['best_valid_loss']\n",
        "    epoch_start =checkpoint['epoch_start']\n",
        "    \n",
        "    net.train()\n",
        "    print('model loaded ...' )\n",
        "\n",
        "\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/checkpoints/vgg16-397923af.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b1209ce6840045dd8b30ad354299c027",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=553433881.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRy2TFvuntBz",
        "colab_type": "text"
      },
      "source": [
        "##### Model Training and saving after each epoch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydia0AxvXzpl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img_dis_step = 100\n",
        "train_loss_dis_step = 10\n",
        "\n",
        "for epoch in range(epoch_start,epoch_start+150):\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_num, shuffle=True, drop_last=True)\n",
        "    net.train()\n",
        "    for train_idx, train_data in enumerate(train_loader,0):\n",
        "        img, gt, label,count = train_data\n",
        "\n",
        "        img = img.to(device)\n",
        "        gt = gt.to(device)\n",
        "        label = label.to(device)\n",
        "        count = count.to(device)\n",
        "\n",
        "        preds = net(img)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # backward + optimize\n",
        "        loss = get_loss_batch(preds, gt,label, count)/batch_num\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        loss_hist.append(loss)\n",
        "\n",
        "        if (train_idx%train_loss_dis_step == 0):\n",
        "            print('epoch %d - iteration %d ---- loss %.6f' %(epoch, train_idx, loss))\n",
        "\n",
        "        \n",
        "        if ((train_idx)%img_dis_step == 0):\n",
        "            plt.plot(loss_hist)\n",
        "            plt.title(\"Training Loss\")            \n",
        "            decoded_preds,fin_class, fin_score = decode_pred_data(preds[0])\n",
        "            visualize_tensor([img[0]], decoded_preds, gt[0][:count[0]])\n",
        "            plt.show()\n",
        "            print(fin_class)\n",
        "            print(fin_score)\n",
        "\n",
        "    \n",
        "    print(\"----- validation step --------\")\n",
        "    valid_loader = DataLoader(valid_dataset, batch_size=batch_num, shuffle=True, drop_last=True)\n",
        "    net.eval()\n",
        "    valid_loss = 0\n",
        "    for valid_idx, valid_data in enumerate(valid_loader):\n",
        "        img, gt, label,count = valid_data\n",
        "        img = img.to(device)\n",
        "        gt = gt.to(device)\n",
        "        label = label.to(device)\n",
        "        count = count.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            preds = net(img)\n",
        "        valid_loss = valid_loss + (get_loss_batch(preds, gt,label, count)/batch_num)\n",
        "\n",
        "        if (valid_idx == 0):\n",
        "            decoded_preds,fin_class, fin_score = decode_pred_data(preds[0])\n",
        "            visualize_tensor([img[0]], decoded_preds, gt[0][:count[0]])\n",
        "            plt.show()\n",
        "            print(fin_class)\n",
        "            print(fin_score)\n",
        "\n",
        "    valid_loss = valid_loss/len(valid_dataset)\n",
        "    valid_hist.append(valid_loss)\n",
        "    print(valid_loss)\n",
        "    plt.plot(valid_hist)\n",
        "    plt.title(\"Valid Loss\")      \n",
        "    plt.show()\n",
        "\n",
        "    PATH = 'drive/My Drive/saved_models/current_'  + opt + '.pt'\n",
        "    net.train()\n",
        "    torch.save({\n",
        "        'net_state_dict': net.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'loss_hist':loss_hist,\n",
        "        'valid_hist':valid_hist,\n",
        "        'best_valid_loss':best_valid_loss,\n",
        "        'epoch_start':epoch\n",
        "      }, PATH)\n",
        "\n",
        "\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        net.train()\n",
        "        print(\"Found new best :) \")\n",
        "        PATH = 'drive/My Drive/saved_models/best_'  + opt + '.pt'\n",
        "        torch.save({\n",
        "            'net_state_dict': net.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss_hist':loss_hist,\n",
        "            'valid_hist':valid_hist,\n",
        "            'best_valid_loss':best_valid_loss,\n",
        "            'epoch_start':epoch\n",
        "        }, PATH)\n",
        "        \n",
        "        \n",
        "\n",
        "\n",
        "    print(\"-------------------------------\")\n",
        "            \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wj_O3E5n3Bu",
        "colab_type": "text"
      },
      "source": [
        "#### Model testing on a single image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MumCWOy1n7m8",
        "colab_type": "text"
      },
      "source": [
        "###### Load the model best saved"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sePJS8hfi-LS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "opt = 'Adam'\n",
        "load_model = 'drive/My Drive/saved_models/best_'  + opt + '.pt'\n",
        "\n",
        "extractor = base_model_vgg16(10)\n",
        "net = YOLONetwork(extractor).to(device)\n",
        "loss_hist = []\n",
        "valid_hist = []\n",
        "best_valid_loss = 100000\n",
        "if opt =='SGD':\n",
        "    optimizer = optim.SGD(net.parameters(), lr=0.001)\n",
        "if opt == 'Adam':\n",
        "    optimizer = optim.Adam(net.parameters(), lr=0.00001)\n",
        "\n",
        "epoch_start = 0\n",
        "\n",
        "if load_model != '':\n",
        "    print('loading model ... ')\n",
        "    checkpoint = torch.load(load_model, map_location=device)\n",
        "    net.load_state_dict(checkpoint['net_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    loss_hist = checkpoint['loss_hist']\n",
        "    valid_hist = checkpoint['valid_hist']\n",
        "    best_valid_loss = checkpoint['best_valid_loss']\n",
        "    epoch_start =checkpoint['epoch_start']\n",
        "    \n",
        "    \n",
        "    print('model loaded ...' )\n",
        "\n",
        "net.eval()\n",
        "print('epoch', epoch_start)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXsf8ouqn_vx",
        "colab_type": "text"
      },
      "source": [
        "###### Run the model on image on eval mode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8gWFgGTfdyb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img = Image.open('drive/My Drive/saved_models/plant.jpg')\n",
        "img = transform(img)\n",
        "img = img.unsqueeze(0)\n",
        "\n",
        "with torch.no_grad():\n",
        "    preds = net(img.to(device))\n",
        "\n",
        "decoded_preds, fin_score,fin_class = decode_pred_data(preds[0])\n",
        "visualize_tensor([img[0]], '',decoded_preds, fin_class, fin_score)\n",
        "plt.show()\n",
        "print(fin_class)\n",
        "print(fin_score)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}